{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering NFL players' data using Scrapy\n",
    "\n",
    "The following document is an easy and simple explanation on how to use spiders to Gather a NFL or any other kind of dataset using Scrapy's resources, then use QGIS framework to make a geospatial Analysis on player's Birthplace and College career.\n",
    "\n",
    "**OBSERVATION**: This document has the only purpose to teach how the code works, it does not have the intention to be run on Jupyter Notebook, since you need all the scrapy auxiliar files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Collect NFL Players' Personal data using the NFL stats page and personal bio available on [NFL main Page](nfl.com) and correlate their Birthplaces and College graduation with the Average income in the U.S.\n",
    "\n",
    "**Hypothesis**: Hall of Fame players birthplace are directly connected with the income of their State/City\n",
    "\n",
    "The first step is to know how to get the players' personal info.\n",
    "\n",
    "To access any stats, it is necessary to acess the [Players' stats](http://www.nfl.com/stats/player) , the page should look just like the image below.\n",
    "\n",
    "![Players stats for the 2019 Season](images/players.png)\n",
    "\n",
    "And then Just click on any player name to acess its personal info, and stats for the current NFL season. The amount of personal info will vary on which year you are looking for, but basically there are two structures available:\n",
    "\n",
    "1. Players active on the NFL\n",
    "    - Height   \n",
    "    - Weight\n",
    "    - Age\n",
    "    - Born (Date and place of birth)\n",
    "    - College\n",
    "    - Experience\n",
    "    - High School\n",
    "\n",
    "![Personal Info](images/tb12.png)\n",
    "\n",
    "2. Retired Players \n",
    "\n",
    "    - Height\n",
    "    - Weight\n",
    "    - Age\n",
    "    - Born (Date and place of birth)\n",
    "    - College\n",
    "    - Experience\n",
    "    - Hall of Fame Induction (If on)\n",
    "    \n",
    "![Personal Info](images/dm13.png)\n",
    "\n",
    "\n",
    "### Complementary Data\n",
    "\n",
    "To complement the information about, it was collected data about the NFL team's stadiums, that can be available on [Sport League Maps](https://sportleaguemaps.com/football/nfl/). From that website it was collected:\n",
    "\n",
    "- Team\n",
    "- Stadium Name\n",
    "- Location (City, State)\n",
    "\n",
    "![Stadium Info](images/stadiums.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy\n",
    "\n",
    "Scrapy is an open-source framework that eases the process of 'crawling' and 'scraping' from the web, providing useful functions and tools that increases the speed of that search.\n",
    "\n",
    "To install Scrapy is necessary the [Anaconda Navigator](http://docs.continuum.io/anaconda/install/), after installed, just type this command to have the scrapy framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new project just type on terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy startproject nameofyourproject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that you should have a new folder with the *spider* subfolder, that's the place where you should put your files to crawl on the web and gather your data.\n",
    "\n",
    "### Spider's Structure\n",
    "\n",
    "The Spider is divided into 2 structures:\n",
    "\n",
    "- start_requests: Region where you insert all the links that you are going to make your spider crawl, depending on the *parse* algorithm, you can have lots of pages or just one home page to make the spider crawl.\n",
    "\n",
    "- parse: Area where the rules to follow on the web pages are available,that can be click on certain link, collect some text, download images, just like an human acessing, but in an automated way and in large scale of collection, saving time and handwork.\n",
    "    - External parser: Responsible to handle the access on the first webpage (Stats) and redirect to the second page (Personal Bio).\n",
    "    - Internal parser: Responsible to find the personal info and store on an appropiate place.\n",
    "    \n",
    "The same is applied to the Stadium website, but withou an External Parser, since all the data is located on the Home Page, avoid the case of accessing other hyperlinks.\n",
    "\n",
    "### HTML Inspection\n",
    "\n",
    "To start scraping from the internet, first it is necessary to study the target website, find what are the elements that you want to capture and how to get to it, also it is necessary to know a thing or two about HTML and CSS, to identify the tags, classes and ID's which the data you want is. To do that you can use any browser with the option *\"Inspect Element\"* on it.\n",
    "\n",
    "\n",
    "![HTML Inspection](images/inspect.png)\n",
    "\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The first step on your spider is to create the class that is going to shape its behaviour to get the data, each project can have just one spider name on it, so take care on naming your spiders correctly. The code above show how it is created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class NFLSpider(scrapy.Spider): \n",
    "    \n",
    "    name = 'nfl'\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is necessary to define the pages to access inside the NFLSpider class, on this case it was used a generator to each Passing stat on NFL post-season since 1933 until the last season. On each iteration a season website will be passed to the *parser* to process the rules and collect the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def start_requests(self):\n",
    "        urls = (f'http://www.nfl.com/stats/categorystats?tabSeq=0&statisticCategory=PASSING&conference=null&season={year}&seasonType=POST&d-447263-s=PASSING_YARDS&d-447263-o=2&d-447263-n=1'\n",
    "            for year in range(1933, datetime.datetime.now().year))\n",
    "        \n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Parser\n",
    "\n",
    "The External Parser is responsible to enter on each link provided on *start_requests* and send the player's info to the Internal parser, with the finality to collect the data. To make this it is necessary to know the \"XPath\", that basically is a reference from the Root div to the desirable element (In this case, the players' bio hyperlink). The code above show how it was made\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def parse(self, response):\n",
    "        \n",
    "        # Extracting all Player links\n",
    "        Player_links = response.xpath('//table[@id=\"result\"]//tbody[1]//tr//td[2]/a/@href').extract()  \n",
    "        for player in Player_links:\n",
    "            info = response.urljoin(player)\n",
    "            yield scrapy.Request(info, callback=self.parse_playerInfo)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal Parser\n",
    "\n",
    "The Internal Parser is the one that makes the magic happens, for each attribute (Name, Bornplace, etc.), its xpath is used to help the spider to find the correct area of collection, with the command *yield*, the data is stored on a temporary database and the format is going to be set when compiling the scrapy's spider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def parse_playerInfo(self,response):\n",
    "        yield {\n",
    "                    'Name' : response.xpath('//div[@class=\"player-info\"]//span[1]/text()').get(default='NA'),\n",
    "                    'Bornplace': response.xpath('//div[@class=\"player-info\"]//p[contains(.,\"Born\")]/text()[2]').get(default='NA'),\n",
    "                    'College':response.xpath('//div[@class=\"player-info\"]//p[contains(.,\"College\")]/text()').get(default='NA'),\n",
    "                    'Experience': response.xpath('//div[@class=\"player-info\"]//p[contains(.,\"Experience\")]/text()').get(default='NA'),\n",
    "                    'HS':response.xpath('//div[@class=\"player-info\"]//p[contains(.,\"School\")]/text()').get(default='NA'),\n",
    "                    'HOF':response.xpath('//div[@class=\"player-info\"]//p[contains(.,\"Fame\")]/text()').get(default='NA')\n",
    "                    \n",
    "                    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code\n",
    "\n",
    "The complete code is available below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "import scrapy\n",
    "import datetime\n",
    "from scrapy.shell import inspect_response\n",
    "\n",
    "\n",
    "class NFLSpider(scrapy.Spider): \n",
    "    \n",
    "    name = 'nfl-spider'\n",
    "    \n",
    "    def start_requests(self):\n",
    "        urls = (f'http://www.nfl.com/stats/categorystats?tabSeq=0&statisticCategory=PASSING&conference=null&season={year}&seasonType=POST&d-447263-s=PASSING_YARDS&d-447263-o=2&d-447263-n=1'\n",
    "            for year in range(1933, datetime.datetime.now().year))\n",
    "        \n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "        \n",
    "    def parse_playerInfo(self,response):\n",
    "        # player_info = response.xpath('//div[@class=\"player-info\"]//p/text()').getall()\n",
    "        # born = //div[@class=\"player-info\"]//p[contains(.,\"Born\")]\n",
    "        yield {\n",
    "                    'Name' : response.xpath('//div[@class=\"player-info\"]//span[1]/text()').get(default='NA'),\n",
    "                    'Bornplace': response.xpath('//div[@class=\"player-info\"]//p[contains(.,\"Born\")]/text()[2]').get(default='NA'),\n",
    "                    'College':response.xpath('//div[@class=\"player-info\"]//p[contains(.,\"College\")]/text()').get(default='NA'),\n",
    "                    'Experience': response.xpath('//div[@class=\"player-info\"]//p[contains(.,\"Experience\")]/text()').get(default='NA'),\n",
    "                    'HS':response.xpath('//div[@class=\"player-info\"]//p[contains(.,\"School\")]/text()').get(default='NA'),\n",
    "                    'HOF':response.xpath('//div[@class=\"player-info\"]//p[contains(.,\"Fame\")]/text()').get(default='NA')\n",
    "                    \n",
    "                    }\n",
    "        \n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Extracting all Player links\n",
    "        Player_links = response.xpath('//table[@id=\"result\"]//tbody[1]//tr//td[2]/a/@href').extract()  \n",
    "        for player in Player_links:\n",
    "            info = response.urljoin(player)\n",
    "            yield scrapy.Request(info, callback=self.parse_playerInfo)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the code for the stadiums:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import scrapy\n",
    "import datetime\n",
    "from scrapy.shell import inspect_response\n",
    "\n",
    "\n",
    "class NFLSpider(scrapy.Spider): \n",
    "    \n",
    "    name = 'stadiums'\n",
    "    \n",
    "    def start_requests(self):\n",
    "        url = 'https://sportleaguemaps.com/football/nfl/'\n",
    "        \n",
    "        yield scrapy.Request(url=url, callback=self.parse)\n",
    "        \n",
    "        \n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Extracting all Player links\n",
    "        stadium_names = response.xpath('//table//tr[not(contains(., \"Logo\"))]//text()').getall()\n",
    "        \n",
    "        for i in range(0, len(stadium_names) - 1, 3):\n",
    "            yield {\n",
    "                    'Team' : stadium_names[i],\n",
    "                    'Name': stadium_names[i+1],\n",
    "                    'Location':stadium_names[i+2]\n",
    "                    }\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Spider\n",
    "\n",
    "\n",
    "To start colecting the info, just go to the scrapy folder and type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl nfl-spider -o output.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- **nfl-spider** is the spider's name\n",
    "- **output.csv** is the file to store the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidying the Data (RStudio)\n",
    "\n",
    "On R-Studio the data was cleaned and organized to be merged in the future with the *geopy* library, the steps taken was listed below:\n",
    "\n",
    "**Observation**: This process can also be equally implemented on Python.\n",
    "\n",
    "1. Read the Spider's output file\n",
    "2. Removed trash from the colector like (':', '\\n', '\\t')\n",
    "3. Organised the Date of Born to european format (DD/MM/YYYY)\n",
    "4. Added commas between the Date and Bornplace to split afterwards\n",
    "5. In case of Bornplaces outside the U.S. it was necessary to add commas between the city and country\n",
    "6. Removed info between parenthesis to ease the gps queries\n",
    "7. Splitted the Bornplace field into (Date of Birth, Place of Birth and State)\n",
    "8. Saving a new csv file named \"NFL-player-bio.csv\"\n",
    "\n",
    "All the code are avaiable below (On R Language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "library(tidyverse)\n",
    "\n",
    "NFL <- read.csv(\"1.csv\")\n",
    "\n",
    "NFL$Bornplace <- gsub(\": \", \"\", NFL$Bornplace  )\n",
    "NFL$Bornplace <- gsub(\"(\\\\n|\\\\t)\", \"\", NFL$Bornplace  )\n",
    "NFL$Bornplace <- gsub(\": \", \"\", NFL$Bornplace  )\n",
    "NFL$Bornplace <- gsub(\"(^[0-9]{1,2})\\\\/([0-9]{1,2})\\\\/([0-9]{4})\", \"\\\\2\\\\/\\\\1\\\\/\\\\3 ,\", NFL$Bornplace)\n",
    "NFL$Bornplace <- gsub(\"(.*?) , (.*) {2,}(.*)$\", \"\\\\1 , \\\\2 , \\\\3\",  NFL$Bornplace)\n",
    "\n",
    "NFL$College <- gsub(\": \", \"\", NFL$College  )\n",
    "NFL$College <- gsub(\"\\\\(.*\\\\)\", \"\", NFL$College )\n",
    "NFL$Experience <- gsub(\": \", \"\", NFL$Experience  )\n",
    "NFL$HS <- gsub(\": \", \"\", NFL$HS  )\n",
    "NFL$HOF <- gsub(\": \", \"\", NFL$HOF  )\n",
    "\n",
    "NFL <- NFL %>% separate(Bornplace, c('Date_Birth', 'Place_Birth', \"State\"), sep = \" , \" )\n",
    "\n",
    "write.csv(NFL, file = \"NFL-player-bio.csv\", row.names = F)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting the data to geospatial providers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting the data, it is necessary to transform all that addresses, that are mostly formed of textual elements, into GPS coordinates (Latitude & Longitude), for that it was used an library called [Geopy](https://geopy.readthedocs.io/en/stable/), a simple library that has lots of Geocoders (some paid, some free) available for making this transformation.\n",
    "\n",
    "The process(both Player bio and stadium location) is summarised into:\n",
    "\n",
    "1. Load Preprocessed CSV file\n",
    "2. Unite the State and City (cities with the same name on different states may cause some problems)\n",
    "3. Merge the \"University\" into the College Name (Unis like 'Florida' may cause some problems)\n",
    "4. Set the GeoCoder (Used ArcGIS)\n",
    "5. Get full coordinates for birthplace and college on a new columns and split them into two\n",
    "6. Remove redundant columns (Location, Coordinate tuple)\n",
    "7. Save to a new CSV file\n",
    "\n",
    "**Important**: [It is necessary to limit the queries to 1 per second to not be blocked by the GeoCoders](https://geopy.readthedocs.io/en/stable/#usage-with-pandas)\n",
    "\n",
    "The full code is available below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from functools import partial\n",
    "\n",
    "from geopy.geocoders import ArcGIS\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "NFL = pd.read_csv('NFL-player-bio.csv')\n",
    "NFL['Full Address'] = NFL['Place_Birth'] + ', ' + NFL['State']\n",
    "NFL['College'] = NFL['College'] + \" University\" \n",
    "\n",
    "geolocator = ArcGIS(user_agent=\"NFL-GPS\")\n",
    "\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds= 1)\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "    \n",
    "NFL['Coords_Birth'] = NFL['Full Address'].progress_apply(partial(geocode,timeout = 5)).apply(lambda x: (x.latitude, x.longitude))\n",
    "NFL['Coords_College'] = NFL['College'].progress_apply(partial(geocode,timeout = 5)).apply(lambda x: (x.latitude, x.longitude))\n",
    "\n",
    "NFL[['Latitude_Birth', 'Longitude_Birth']] = pd.DataFrame(NFL['Coords_Birth'].tolist(), index=NFL.index)\n",
    "NFL[['Latitude_College', 'Longitude_College']] = pd.DataFrame(NFL['Coords_College'].tolist(), index=NFL.index)\n",
    "\n",
    "NFL.drop(columns=['Coords_Birth', 'Coords_College', 'Full Address'], inplace= True)\n",
    "\n",
    "NFL.to_csv(\"NFL-GPS.csv\", index=False)\n",
    "\n",
    "\n",
    "# Stadium transformation\n",
    "\n",
    "Stadium = pd.read_csv('Stadium.csv')\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "Stadium['Coords'] = Stadium['Name'].progress_apply(partial(geocode,timeout = 5)).apply(lambda x: (x.latitude, x.longitude))\n",
    "Stadium[['Latitude', 'Longitude']] = pd.DataFrame(Stadium['Coords'].tolist(), index=Stadium.index)\n",
    "\n",
    "Stadium.drop(columns=['Coords'], inplace= True)\n",
    "\n",
    "Stadium.to_csv(\"NFL-GPS-Stadiums.csv\", index=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QGIS\n",
    "\n",
    "The QGIS is the responsible for generate maps using datasets, and now with these two new dataset, it is possible to show on the U.S. map the relationship between NFL Players, Status and [US Household Income Statistics](https://www.kaggle.com/goldenoakresearch/us-household-income-stats-geo-locations)\n",
    "\n",
    "To add a map first is necessary to install the OpenLayers plugin on the QGIS app and just drag-and-drop the CSV files on the application. The Income dataset was transformed into a heatmap \n",
    "\n",
    "Here it is the individual visualization of each Type:\n",
    "\n",
    "### Birth place of Hall of Famers\n",
    "\n",
    "![Birth-HOF](images/HOF-Birth.png)\n",
    "\n",
    "### Hall of Famers Colleges\n",
    "\n",
    "![College-HOF](images/HOF-College.png)\n",
    "\n",
    "### NFL Stadium Locations\n",
    "\n",
    "![Stadiums](images/Stadiums-QGIS.png)\n",
    "\n",
    "\n",
    "# Analysis\n",
    "\n",
    "The first point to observe is the East coast dominance in Hall of Famers, even with the presence of big cities, like Los Angeles and Seattle. \n",
    "\n",
    "Even though the East is crowded with stadiums, few Hall of Famers were born in the far East zones (Boston, Washington and New york), maybe because people born on these places tend to focus on Business areas than Sports. \n",
    "\n",
    "![East coast](images/East-Coast.png)\n",
    "\n",
    "The most crowded areas are near Philadelphia and Texas/Louisiana state, which has less economical power than the biggest cities (New York and Los Angeles).\n",
    "\n",
    "![East coast](images/West-Coast.png)\n",
    "\n",
    "\n",
    "It is good to observe that mostly of HOFers were born far from the the big cities and that simpler life outside the metropolis should've helped to develop the love for the sport and make them follow a career through it.\n",
    "\n",
    "Another interesting fact is that on the University time, players tend to get more away from the NFL's stadiums locations to play on furthest colleges and then finally come to the city centers following a professional career on the NFL.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "With that it was proved that the combo Scrapy + Geopy + QGIS is really good to transform textual data from the web into Geospatial elements, providing some new insights about how players know the sport and how they develop until the professional career. Due the lack of time, it was not possible to make more complex hypothesis or analysis, but the Hypothesis\n",
    "\n",
    "The final map is here:\n",
    "\n",
    "![NFL](images/USA.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
